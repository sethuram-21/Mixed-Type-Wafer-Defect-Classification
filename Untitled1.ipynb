{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfca9f6-3f1f-44d4-9def-3c2eadc3d804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in /home/sethu/anaconda3/lib/python3.12/site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b292f06-fc47-475a-a675-d92a62de2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08fd91f9-c17f-40cc-b3b4-0aacb0b4745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"data\"\n",
    "output = \"data split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ace41b5-b11b-4fa5-89bf-e42469fa2d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 38015 files [00:06, 5943.60 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio(input, output=output, seed=1337, ratio=(0.72,0.08,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ca8eb8-eefd-4e1d-ada7-fb411a6bc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b12882d-806d-4351-837a-aecad067a995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c4',\n",
       " 'c19',\n",
       " 'c7',\n",
       " 'c2',\n",
       " 'c15',\n",
       " 'c12',\n",
       " 'c37',\n",
       " 'c29',\n",
       " 'c26',\n",
       " 'c31',\n",
       " 'c30',\n",
       " 'c24',\n",
       " 'c3',\n",
       " 'c35',\n",
       " 'c22',\n",
       " 'c11',\n",
       " 'c38',\n",
       " 'c25',\n",
       " 'c1',\n",
       " 'c20',\n",
       " 'c28',\n",
       " 'c33',\n",
       " 'c9',\n",
       " 'c27',\n",
       " 'c32',\n",
       " 'c8',\n",
       " 'c5',\n",
       " 'c10',\n",
       " 'c36',\n",
       " 'c13',\n",
       " 'c21',\n",
       " 'c16',\n",
       " 'c17',\n",
       " 'c34',\n",
       " 'c18',\n",
       " 'c23',\n",
       " 'c6',\n",
       " 'c14']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data split/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c787f95b-a96a-4281-afaa-356198e50df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path=\"./data split/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0b19e-8c06-4cc3-9858-63f8e8d51423",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.Resize((52,52)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a2c360-9faa-42fc-885a-0d75848ede00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder( root = training_dataset_path, transform = training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa311fe-c0eb-41c0-9a3b-faf51aede175",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = 32 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b4f2b02-f73a-4765-bff6-c5b697325478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    # Compute the mean and standard deviation of all pixels in the dataset\n",
    "    num_pixels = 0\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        num_pixels += batch_size * height * width\n",
    "        mean += images.mean(axis=(0, 2, 3)).sum()\n",
    "        std += images.std(axis=(0, 2, 3)).sum()\n",
    "\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e70b3527-041b-4983-8b20-dc55f53d63c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,std = get_mean_std(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db7fdd5d-48fb-4dd9-a7a5-7f0ba09e5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d1555fb-735e-4b6e-aa36-f809aa456154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './data split/train'\n",
    "test_dataset_path = './data split/test'\n",
    "val_dataset_path = './data split/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37baf31e-bbdc-4e0e-9e4a-272da7087bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((52,52)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((52,52)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((52,52)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a7dee68-6dca-47bf-9e64-f7f25489b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root = train_dataset_path, transform = train_transforms)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root = test_dataset_path, transform = test_transforms)\n",
    "val_dataset = torchvision.datasets.ImageFolder(root = val_dataset_path, transform = val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca97f83b-13c0-4872-9f37-ff448e3ecd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7bcf9ff-6dce-47cb-aa5c-b71e115e8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda\"\n",
    "    else : \n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "676f0ca9-a5fe-47e7-b7b2-397b9a736ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ratio=16, kernel_size=7):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "        self.channel_attention = ChannelAttention(out_channels, ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)  # Feature Maps (F)\n",
    "        x = x * self.channel_attention(x)  # Channel Refined Maps (F')\n",
    "        x = x * self.spatial_attention(x)  # Fully Refined Maps (F'')\n",
    "        return x\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvModule, self).__init__()\n",
    "        self.conv1 = ConvBlock(1, 256)  # Assuming input image has 1 channel\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ConvBlock(256, 256),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            ConvBlock(256, 128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention_blocks = nn.ModuleList(\n",
    "            [AttentionBlock(in_channels if i == 0 else out_channels, out_channels) for i in range(8)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for attention_block in self.attention_blocks:\n",
    "            x = attention_block(x)\n",
    "            outputs.append(x)\n",
    "        return sum(outputs)\n",
    "\n",
    "class ClassificationModule(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(ClassificationModule, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(in_features, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class AttentionAugmentedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=38):\n",
    "        super(AttentionAugmentedCNN, self).__init__()\n",
    "        self.conv_module = ConvModule()\n",
    "        self.attention_module = AttentionModule(128, 128)  # Assuming output of ConvModule is 128 channels\n",
    "        self.classification_module = ClassificationModule(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_module(x)\n",
    "        x = self.attention_module(x)\n",
    "        x = self.classification_module(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1db6e188-e29b-4e72-b8bb-52a1ddefdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = F.one_hot(targets, num_classes=inputs.size(-1))\n",
    "        log_p = F.log_softmax(inputs, dim=-1)\n",
    "        p = torch.exp(log_p)\n",
    "        focal_loss = -self.alpha * (1 - p) ** self.gamma * log_p\n",
    "        focal_loss = focal_loss * targets\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f94f380-8804-4bb8-805a-2106da7d9453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] Loss: 0.0149 Accuracy: 69.05%\n",
      "Validation Loss: 0.0070 Accuracy: 81.78%\n",
      "Epoch [2] Loss: 0.0036 Accuracy: 91.29%\n",
      "Validation Loss: 0.0025 Accuracy: 93.78%\n",
      "Epoch [3] Loss: 0.0026 Accuracy: 93.76%\n",
      "Validation Loss: 0.0014 Accuracy: 96.55%\n",
      "Epoch [4] Loss: 0.0022 Accuracy: 94.62%\n",
      "Validation Loss: 0.0018 Accuracy: 96.05%\n",
      "Epoch [5] Loss: 0.0022 Accuracy: 94.78%\n",
      "Validation Loss: 0.0025 Accuracy: 93.91%\n",
      "Epoch [6] Loss: 0.0017 Accuracy: 95.84%\n",
      "Validation Loss: 0.0024 Accuracy: 95.49%\n",
      "Epoch [7] Loss: 0.0019 Accuracy: 95.58%\n",
      "Validation Loss: 0.0022 Accuracy: 95.23%\n",
      "Epoch [8] Loss: 0.0015 Accuracy: 96.48%\n",
      "Validation Loss: 0.0012 Accuracy: 97.24%\n",
      "Epoch [9] Loss: 0.0015 Accuracy: 96.35%\n",
      "Validation Loss: 0.0017 Accuracy: 95.66%\n",
      "Epoch [10] Loss: 0.0013 Accuracy: 96.86%\n",
      "Validation Loss: 0.0017 Accuracy: 96.38%\n",
      "Test Accuracy: 96.95%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = AttentionAugmentedCNN().to(device)\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets) \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(f'Epoch [{epoch+1}] Loss: {running_loss/len(train_loader):.4f} Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print(f'Validation Loss: {running_loss/len(val_loader):.4f} Accuracy: {100.*correct/total:.2f}%')\n",
    "    return running_loss / len(val_loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, criterion, optimizer, epoch)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "print(f'Test Accuracy: {100.*correct/total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b30d332f-0697-49af-be7e-1dc7b0c4a770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model = AttentionAugmentedCNN()  \n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))  \n",
    "model = model.to(device)  \n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    all_images.append(images)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_images = torch.cat(all_images, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "index = random.randint(0, len(all_images) - 1)\n",
    "image = all_images[index]\n",
    "label = all_labels[index]\n",
    "\n",
    "image_tensor = image.unsqueeze(0) \n",
    "image_tensor = image_tensor.to(device)  \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "image_np = image.cpu().numpy().transpose((1, 2, 0))  \n",
    "mean = np.array([0.485, 0.456, 0.406])  \n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "image_np = std * image_np + mean \n",
    "image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "\n",
    "plt.imshow(image_np)\n",
    "plt.title(f'Predicted: {predicted.item()}, True: {label.item()}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7134bb-c856-491d-8807-77300917b756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48656e-a27f-4872-9536-748cab5beea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
